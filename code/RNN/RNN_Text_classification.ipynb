{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_RNN_CLS.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "2ew7HTbPpCJH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "imdb = keras.datasets.imdb\n",
        "\n",
        "#(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LF5w-eqejZh5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import collections\n",
        "def ReadListAndDictFromFile(readFileName):\n",
        "    with open(readFileName, \"r\",encoding='UTF-8') as f:\n",
        "        readedList =  json.loads(f.read())\n",
        "        return readedList\n",
        "        \n",
        "#for example\n",
        "CNN_list = ReadListAndDictFromFile(\"CNN_sentence_list.txt\")\n",
        "label_list = ReadListAndDictFromFile(\"label_list.txt\")\n",
        "print(CNN_list[0][1])\n",
        "minLength = 20\n",
        "with open('sorted_word2id.txt', 'r') as fp:\n",
        "    word2id = json.loads(fp.read())\n",
        "word_dict = dict(collections.Counter(word2id).most_common()[:-10000-1:-1])\n",
        "\n",
        "new_CNN_list = []\n",
        "new_label_list = []\n",
        "for i in range(len(label_list)):\n",
        "    for j in range(len(CNN_list[i])):\n",
        "        if len(CNN_list[i][j]) < minLength:\n",
        "            continue\n",
        "        else:    \n",
        "            for k in range(len(CNN_list[i][j])):\n",
        "                if CNN_list[i][j][k] in word_dict:\n",
        "                    CNN_list[i][j][k] = word_dict[CNN_list[i][j][k]]+3\n",
        "                else:\n",
        "                    CNN_list[i][j][k] = 2\n",
        "            new_CNN_list.append(CNN_list[i][j])\n",
        "            if label_list[i] < 2.1:\n",
        "                new_label_list.append(0.0)\n",
        "            elif label_list[i] < 3.1:\n",
        "                new_label_list.append(1.0)  \n",
        "            else:\n",
        "                new_label_list.append(2.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ji0SSqBwjZh8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reCount():\n",
        "    new_CNN_list = []\n",
        "    new_label_list = []\n",
        "    for i in range(len(label_list)):\n",
        "        for j in range(len(CNN_list[i])):\n",
        "            if len(CNN_list[i][j]) < minLength:\n",
        "                continue\n",
        "            else:    \n",
        "                for k in range(len(CNN_list[i][j])):\n",
        "                    if CNN_list[i][j][k] in word_dict:\n",
        "                        word_dict[CNN_list[i][j][k]] += 1\n",
        "                    else:\n",
        "                        word_dict[CNN_list[i][j][k]] =1\n",
        "                new_CNN_list.append(CNN_list[i][j])\n",
        "                if label_list[i] < 2.1:\n",
        "                    new_label_list.append(0.0)\n",
        "                elif label_list[i] < 4.1:\n",
        "                    new_label_list.append(1.0)  \n",
        "                else:\n",
        "                    new_label_list.append(2.0)  \n",
        "def ordered_list(word_dict):    \n",
        "    cnt = collections.Counter(word_dict)\n",
        "    codex = {}\n",
        "    print(dict(cnt.most_common(100)))\n",
        "    words_ord = list(dict(cnt))\n",
        "    for k, i in enumerate(dict(cnt.most_common()).keys()):\n",
        "        codex[i] = k\n",
        "    with open('sorted_word2id.txt', 'w') as fp:\n",
        "        json.dump(codex, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BNhxb_nZjZh_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = new_CNN_list[2000:100000]\n",
        "train_labels = new_label_list[2000:100000]\n",
        "test_data = new_CNN_list[:2000]\n",
        "test_labels = new_label_list[:2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tr5s_1alpzop",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "#word_index = {str(val):int(key) for (key, val) in word_dict.items()}\n",
        "word_index = word_dict\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "#word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "decode_review(test_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "USSSBnkE-lky",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
        "n, bins, patches = plt.hist(x=[len(s)/10 for s in train_data], bins='auto', color='#0504aa',\n",
        "                            alpha=0.7, rwidth=0.85)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('My Very Own Histogram')\n",
        "plt.text(23, 45, r'$\\mu=15, b=3$')\n",
        "maxfreq = n.max()\n",
        "# Set a clean upper y-axis limit.\n",
        "plt.ylim(ymax=np.ceil(maxfreq / 100) * 100 if maxfreq % 100 else maxfreq + 100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4IdYbEYAr1Ct",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "EMBEDDING_LENGTH = 100\n",
        "RNN_UNITS = 100\n",
        "BATCH_SIZE = 128\n",
        "CLASSES = 3\n",
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "x_val = train_data[:10000]\n",
        "partial_x_train = train_data[10000:]\n",
        "\n",
        "y_val = keras.utils.to_categorical(train_labels[:10000])\n",
        "partial_y_train = keras.utils.to_categorical(train_labels[10000:])\n",
        "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
        "print(partial_x_train.shape, y_val.shape, train_data.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9Yu7Y-oRBYvu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstm_model = keras.Sequential()\n",
        "lstm_model.add(keras.layers.Embedding(vocab_size, EMBEDDING_LENGTH))\n",
        "lstm_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units = RNN_UNITS)))\n",
        "lstm_model.add(tf.keras.layers.Dropout(0.5))\n",
        "lstm_model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
        "lstm_model.add(keras.layers.Dense(CLASSES, activation=tf.nn.sigmoid))\n",
        "lstm_model.compile(optimizer=tf.train.AdamOptimizer(),              loss='binary_crossentropy',              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ykn2biS4tpK3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence_input = keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "embedded_sequences = keras.layers.Embedding(vocab_size, EMBEDDING_LENGTH)(sequence_input)\n",
        "\n",
        "tf.expand_dims(embedded_sequences, 3)\n",
        "\n",
        "#cnn = tf.keras.layers.Conv2D(100, (3, EMBEDDING_LENGTH), (1, 1), activation = 'relu', kernel_regularizer = keras.regularizers.l2(3))(embedded_sequences)\n",
        "#dropout = tf.keras.layers.Dropout(0.5)(cnn)\n",
        "preds = keras.layers.Dense(CLASSES, activation='softmax')(embedded_sequences)\n",
        "cnn_model = tf.keras.Model(sequence_input, preds)\n",
        "#cnn_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "#cnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "e1WkDXyUtpLA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#lstm_model.add(tf.keras.layers.Conv2D(100, (3, 64), (1, 1), activation = 'relu', kernel_regularizer = keras.regularizers.l2(3)))\n",
        "sequence_input = keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = keras.layers.Embedding(vocab_size, EMBEDDING_LENGTH)(sequence_input)\n",
        "l_cov1= keras.layers.Conv1D(EMBEDDING_LENGTH, 3, activation='relu')(embedded_sequences)\n",
        "l_pool1 = keras.layers.MaxPooling1D(5)(l_cov1)\n",
        "l_cov2 = keras.layers.Conv1D(EMBEDDING_LENGTH, 3, activation='relu')(l_pool1)\n",
        "l_pool2 = keras.layers.MaxPooling1D(5)(l_cov2)\n",
        "#l_cov3 = keras.layers.Conv1D(EMBEDDING_LENGTH, 3, activation='relu')(l_pool2)\n",
        "#l_pool3 = keras.layers.MaxPooling1D(3)(l_cov3)  # global max pooling\n",
        "l_flat = keras.layers.Flatten()(l_pool2)\n",
        "l_dense = keras.layers.Dense(EMBEDDING_LENGTH, activation='relu')(l_flat)\n",
        "preds = keras.layers.Dense(CLASSES, activation='softmax')(l_dense)\n",
        "akshat_cnn_model = tf.keras.Model(sequence_input, preds)\n",
        "#akshat_cnn_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "#akshat_cnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Mr0GP-cQ-llN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence_input = keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = keras.layers.Embedding(vocab_size, EMBEDDING_LENGTH)(sequence_input)\n",
        "l_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(RNN_UNITS))(embedded_sequences)\n",
        "preds = keras.layers.Dense(CLASSES, activation='softmax')(l_lstm)\n",
        "richard_model = tf.keras.Model(sequence_input, preds)\n",
        "#richard_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "#richard_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tXSGrjWZ-llW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = lstm_model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=3,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9EEGuDVuzb5r"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model\n",
        "\n",
        "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zOMKywn4zReN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = lstm_model.evaluate(test_data, keras.utils.to_categorical(test_labels))\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p35z-RhAjZic",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(threshold=np.nan)\n",
        "f = open(\"output-predict\",'w')  \n",
        "f2 = open(\"output-target\",'w')\n",
        "predict = lstm_model.predict(tf.convert_to_tensor(x_val), steps = 1)\n",
        "target = y_val\n",
        "print (predict, file=f)  \n",
        "print (target, file=f2)\n",
        "f.close()\n",
        "f2.close()\n",
        "\n",
        "predict = np.transpose(predict)\n",
        "target = np.transpose(target)\n",
        "tp = [np.sum(np.dot(p, t)) for p, t in zip(predict, target)]\n",
        "tn = [np.sum(np.dot((1-p), (1-t))) for p, t in zip(predict, target)]\n",
        "fp = [np.sum(np.dot(p, (1-t))) for p, t in zip(predict, target)]\n",
        "fn = [np.sum(np.dot((1-p), t)) for p, t in zip(predict, target)]\n",
        "eva = np.array([tp,tn,fp,fn])\n",
        "precision = np.divide(tp, np.add(tp, fp))\n",
        "recall  = np.divide(tp, np.add(tp, fn))\n",
        "f1_score = (2/((1/precision)+(1/recall)))\n",
        "result = np.array([precision,recall,f1_score])\n",
        "print(eva)\n",
        "print(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "z1iEXVTR0Z2t"
      },
      "cell_type": "markdown",
      "source": [
        "This fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5KggXVeL-llZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a graph of accuracy and loss over time\n",
        "\n",
        "`model.fit()` returns a `History` object that contains a dictionary with everything that happened during training:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VcvSXvhp-llb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nRKsqL40-lle"
      },
      "cell_type": "markdown",
      "source": [
        "There are four entries: one for each monitored metric during training and validation. We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nGoYf2Js-lle",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6hXx-xOv-llh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.clf()   # clear figure\n",
        "acc_values = history_dict['acc']\n",
        "val_acc_values = history_dict['val_acc']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oFEmZ5zq-llk"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy.\n",
        "\n",
        "Notice the training loss *decreases* with each epoch and the training accuracy *increases* with each epoch. This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\n",
        "\n",
        "This isn't the case for the validation loss and accuracy—they seem to peak after about twenty epochs. This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations *specific* to the training data that do not *generalize* to test data.\n",
        "\n",
        "For this particular case, we could prevent overfitting by simply stopping the training after twenty or so epochs. Later, you'll see how to do this automatically with a callback."
      ]
    }
  ]
}